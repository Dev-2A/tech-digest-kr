import json
import re
from datetime import datetime, timezone
from pathlib import Path
from time import mktime

import feedparser
import httpx

from src.collectors.models import FeedEntry


class RSSCollector:
    """RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•˜ì—¬ FeedEntry ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    
    def __init__(self, feeds_path: str | None = None):
        if feeds_path is None:
            feeds_path = str(
                Path(__file__).resolve().parent.parent.parent / "config" / "feeds.json"
            )
        self.feeds_path = feeds_path
        self.feeds_config = self._load_feeds_config()
    
    def _load_feeds_config(self) -> list[dict]:
        with open(self.feeds_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return [feed for feed in data["feeds"] if feed.get("enabled", True)]
    
    def _clean_html(self, raw_html: str) -> str:
        """HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
        clean = re.sub(r"<[^>]+>", "", raw_html)
        clean = re.sub(r"\s+", " ", clean).strip()
        return clean
    
    def _parse_published(self, entry) -> datetime:
        """ë°œí–‰ì¼ì„ datetimeìœ¼ë¡œ íŒŒì‹±"""
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            return datetime.fromtimestamp(
                mktime(entry.published_parsed), tz=timezone.utc
            )
        if hasattr(entry, "updated_parsed") and entry.updated_parsed:
            return datetime.fromtimestamp(
                mktime(entry.updated_parsed), tz=timezone.utc
            )
        return datetime.now(tz=timezone.utc)
    
    def _extract_tags(self, entry) -> list[str]:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ íƒœê·¸/ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        tags = []
        if hasattr(entry, "tags"):
            for tag in entry.tags:
                term = tag.get("term", "").strip().lower()
                if term:
                    tags.append(term)
        return tags
    
    def _extract_content(self, entry) -> str:
        """ë³¸ë¬¸ ì¶”ì¶œ (content > summary > description ìš°ì„ ìˆœìœ„)"""
        if hasattr(entry, "content") and entry.content:
            raw = entry.content[0].get("value", "")
        elif hasattr(entry, "summary") and entry.summary:
            raw = entry.summary
        elif hasattr(entry, "description") and entry.description:
            raw = entry.description
        else:
            raw = ""
        return self._clean_html(raw)
    
    def collect_feed(self, feed_config: dict) -> list[FeedEntry]:
        """ë‹¨ì¼ í”¼ë“œì—ì„œ ê¸€ ëª©ë¡ ìˆ˜ì§‘"""
        entries = []
        try:
            response = httpx.get(feed_config["url"], timeout=15, follow_redirects=True)
            parsed = feedparser.parse(response.text)
            
            for entry in parsed.entries:
                feed_entry = FeedEntry(
                    title=entry.get("title", "ì œëª© ì—†ìŒ"),
                    url=entry.get("link", ""),
                    author=entry.get("author", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    published=self._parse_published(entry),
                    content=self._extract_content(entry),
                    platform=feed_config["platform"],
                    feed_name=feed_config["name"],
                    tags=self._extract_tags(entry),
                )
                entries.append(feed_entry)
            
            print(f"  âœ… {feed_config['name']}: {len(entries)}ê±´ ìˆ˜ì§‘")
        
        except Exception as e:
            print(f"  âŒ {feed_config['name']}: ìˆ˜ì§‘ ì‹¤íŒ¨ - {e}")
        
        return entries
    
    def collect_all(self) -> list[FeedEntry]:
        """ë“±ë¡ëœ ëª¨ë“  í”¼ë“œì—ì„œ ê¸€ ìˆ˜ì§‘"""
        all_entries = []
        print(f"ğŸ“¡ {len(self.feeds_config)}ê°œ í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘...")
        
        for feed_config in self.feeds_config:
            entries = self.collect_feed(feed_config)
            all_entries.extend(entries)
        
        print(f"ğŸ“¦ ì´ {len(all_entries)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_entries