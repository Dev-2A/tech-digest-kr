"""ìˆ˜ì§‘ â†’ ìš”ì•½ â†’ íƒœê·¸ ì¶”ì¶œ â†’ ì„ë² ë”© â†’ ë¶„ë¥˜ â†’ ì €ì¥ í†µí•© íŒŒì´í”„ë¼ì¸"""
from datetime import datetime, timezone

from src.collectors.rss_collector import RSSCollector
from src.summarizer.llm_summarizer import LLMSummarizer
from src.tagger.tag_extractor import TagExtractor, TagFilter
from src.embeddings.embedding_service import EmbeddingService, ArticleClassifier
from src.storage.database import Database
from config.settings import settings


class DigestPipeline:
    """Tech Digest KR ì „ì²´ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, db: Database | None = None):
        self.db = db or Database()
        self.collector = RSSCollector()
        self.summarizer = LLMSummarizer()
        self.tag_extractor = TagExtractor()
        self.embedding_service = EmbeddingService()
        self.classifier = ArticleClassifier(self.embedding_service)

    def run(self, skip_existing: bool = True) -> dict:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            skip_existing: Trueë©´ ì´ë¯¸ DBì— ìˆëŠ” ê¸€ì€ ê±´ë„ˆëœ€

        Returns:
            {
                "collected": int,
                "new_articles": int,
                "skipped": int,
                "summarized": int,
                "familiar": int,
                "novel": int,
                "digest": list[dict]
            }
        """
        result = {
            "collected": 0,
            "new_articles": 0,
            "skipped": 0,
            "summarized": 0,
            "familiar": 0,
            "novel": 0,
            "digest": [],
        }

        # === 1ë‹¨ê³„: RSS ìˆ˜ì§‘ ===
        print("\n" + "=" * 60)
        print("ğŸ“¡ [1/5] RSS í”¼ë“œ ìˆ˜ì§‘")
        print("=" * 60)

        entries = self.collector.collect_all()
        result["collected"] = len(entries)

        if not entries:
            print("âš ï¸ ìˆ˜ì§‘ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return result

        # ì¤‘ë³µ í•„í„°ë§
        if skip_existing:
            new_entries = [e for e in entries if not self.db.article_exists(e.url)]
            result["skipped"] = len(entries) - len(new_entries)
            entries = new_entries
            print(f"  ğŸ†• ì‹ ê·œ: {len(entries)}ê±´ | â­ï¸ ê±´ë„ˆëœ€: {result['skipped']}ê±´")

        if not entries:
            print("âœ… ìƒˆë¡œìš´ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return result

        result["new_articles"] = len(entries)

        # === 2ë‹¨ê³„: LLM ìš”ì•½ ===
        print("\n" + "=" * 60)
        print("ğŸ¤– [2/5] LLM 3ì¤„ ìš”ì•½ ìƒì„±")
        print("=" * 60)

        summarized = self.summarizer.summarize_batch(entries)
        result["summarized"] = sum(1 for s in summarized if s["summary"]["success"])

        # === 3ë‹¨ê³„: íƒœê·¸ ì¶”ì¶œ ===
        print("\n" + "=" * 60)
        print("ğŸ·ï¸ [3/5] íƒœê·¸ ì¶”ì¶œ")
        print("=" * 60)

        articles = []
        for item in summarized:
            entry = item["entry"]
            print(f"  [{entry.title[:40]}...]")

            tags = self.tag_extractor.extract_tags(
                entry.title, entry.content, entry.tags
            )
            print(f"    â†’ {', '.join(tags)}")

            articles.append({
                "entry": entry,
                "summary": item["summary"],
                "tags": tags,
            })

        # === 4ë‹¨ê³„: ì„ë² ë”© + ë¶„ë¥˜ ===
        print("\n" + "=" * 60)
        print("ğŸ§  [4/5] ì„ë² ë”© ìƒì„± + ì½ì€ ê¸€ ê¸°ë°˜ ë¶„ë¥˜")
        print("=" * 60)

        # ì„ë² ë”© ìƒì„±
        texts_for_embedding = []
        for article in articles:
            entry = article["entry"]
            summary_text = article["summary"].get("summary", "")
            tag_text = ", ".join(article["tags"])
            texts_for_embedding.append(f"{entry.title} íƒœê·¸: {tag_text} {summary_text}")

        vectors = self.embedding_service.encode_batch(texts_for_embedding)

        # ì½ì€ ê¸°ë¡ ë¡œë“œ + ë¶„ë¥˜
        read_vectors = self.db.get_read_embeddings()
        if read_vectors is not None:
            self.classifier.update_read_history(read_vectors)
            print(f"  ğŸ“š ì½ì€ ê¸€ {len(read_vectors)}ê±´ì˜ ë²¡í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        else:
            print("  â„¹ï¸ ì½ì€ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ê¸€ì„ 'ìƒˆë¡œìš´ ê¸€'ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.")

        classified = self.classifier.classify(articles)
        result["familiar"] = len(classified["familiar"])
        result["novel"] = len(classified["novel"])

        print(f"  ğŸ”„ ë¹„ìŠ·í•œ ê¸€: {result['familiar']}ê±´")
        print(f"  ğŸ†• ìƒˆë¡œìš´ ê¸€: {result['novel']}ê±´")

        # === 5ë‹¨ê³„: DB ì €ì¥ ===
        print("\n" + "=" * 60)
        print("ğŸ’¾ [5/5] ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        print("=" * 60)

        articles_to_save = []
        for i, article in enumerate(articles):
            entry = article["entry"]
            summary = article["summary"]

            articles_to_save.append({
                "url": entry.url,
                "title": entry.title,
                "author": entry.author,
                "published_at": entry.published.isoformat(),
                "content": entry.content_preview,
                "platform": entry.platform,
                "feed_name": entry.feed_name,
                "tags": article["tags"],
                "summary": summary.get("summary", ""),
                "summary_lines": summary.get("lines", []),
                "embedding": vectors[i],
            })

        save_result = self.db.insert_articles_batch(articles_to_save)
        print(f"  âœ… ì €ì¥: {save_result['inserted']}ê±´ | â­ï¸ ê±´ë„ˆëœ€: {save_result['skipped']}ê±´")

        # ë‹¤ì´ì œìŠ¤íŠ¸ ê¸°ë¡
        self.db.log_digest(
            article_count=len(articles),
            familiar_count=result["familiar"],
            novel_count=result["novel"],
        )

        # === ê´€ì‹¬ íƒœê·¸ í•„í„°ë§ ê²°ê³¼ ===
        interest_tags_db = self.db.get_interest_tags()
        if interest_tags_db:
            interest_tag_list = [t["tag"] for t in interest_tags_db]
        else:
            interest_tag_list = settings.default_interest_tags

        tag_filter = TagFilter(interest_tags=interest_tag_list)

        # ë‹¤ì´ì œìŠ¤íŠ¸ ìƒì„±
        digest = []
        for category, label in [("novel", "ğŸ†• ìƒˆë¡œìš´ ê¸€"), ("familiar", "ğŸ”„ ë¹„ìŠ·í•œ ê¸€")]:
            for item in classified[category]:
                article = item["article"]
                entry = article["entry"]
                relevance = tag_filter.calculate_relevance(article["tags"])

                digest.append({
                    "title": entry.title,
                    "url": entry.url,
                    "author": entry.author,
                    "platform": entry.platform,
                    "tags": article["tags"],
                    "summary_lines": article["summary"].get("lines", []),
                    "category": label,
                    "similarity": item["max_similarity"],
                    "relevance_score": relevance["score"],
                    "matched_tags": relevance["matched_tags"],
                })

        result["digest"] = digest
        return result

    def print_digest(self, result: dict):
        """ë‹¤ì´ì œìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        digest = result.get("digest", [])

        if not digest:
            print("\nğŸ“­ ì˜¤ëŠ˜ì˜ ë‹¤ì´ì œìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return

        print("\n" + "=" * 60)
        print(f"ğŸ“° Tech Digest KR â€” {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        print(f"   ìˆ˜ì§‘ {result['collected']}ê±´ â†’ ì‹ ê·œ {result['new_articles']}ê±´")
        print(f"   ğŸ†• ìƒˆë¡œìš´ ê¸€ {result['novel']}ê±´ | ğŸ”„ ë¹„ìŠ·í•œ ê¸€ {result['familiar']}ê±´")
        print("=" * 60)

        current_category = ""
        for item in digest:
            if item["category"] != current_category:
                current_category = item["category"]
                print(f"\n--- {current_category} ---\n")

            print(f"ğŸ“Œ {item['title']}")
            print(f"   ğŸ‘¤ {item['author']} | ğŸ“¦ {item['platform']}")
            print(f"   ğŸ·ï¸ {', '.join(item['tags'])}")

            if item["summary_lines"]:
                print("   ğŸ“ ìš”ì•½:")
                for j, line in enumerate(item["summary_lines"], 1):
                    print(f"      {j}. {line}")

            if item["matched_tags"]:
                print(f"   ğŸ¯ ê´€ì‹¬ íƒœê·¸ ë§¤ì¹­: {', '.join(item['matched_tags'])}")

            print(f"   ğŸ”— {item['url']}")
            print()